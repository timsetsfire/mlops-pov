{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files were overwritten: {'/opt/fit_output_dir/model-config.yaml', '/opt/fit_output_dir/feature_detail.yaml', '/opt/fit_output_dir/create_pipeline.py', '/opt/fit_output_dir/athena_creds.yaml', '/opt/fit_output_dir/README.md', '/opt/fit_output_dir/azure_creds.yaml', '/opt/fit_output_dir/create_data.py', '/opt/fit_output_dir/artifact.pkl', '/opt/fit_output_dir/custom_model.py', '/opt/fit_output_dir/schema.json', '/opt/fit_output_dir/custom.py'}\n",
      "Success ðŸŽ‰\n",
      "WARNING: looks like host DRUM version doesn't match container DRUM version. This can lead to unexpected behavior.\n",
      "Host DRUM version: drum 1.4.8\n",
      "Container DRUM version: \n",
      "the input device is not a TTY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "drum fit --code-dir ./model \\\n",
    "--input data/10K_Lending_Club_Loans_utf-8.csv \\\n",
    "--output ./model \\\n",
    "--target-type binary \\\n",
    "--target is_bad \\\n",
    "--docker env \n",
    "--verbose\n",
    "--logging-level debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected perf-test mode\n",
      "DRUM performance test\n",
      "Model:      /Users/timothy.whittaker/Desktop/git/ford-mlops/model\n",
      "Data:       /Users/timothy.whittaker/Desktop/git/ford-mlops/data/10K_LC_short.csv\n",
      "# Features: 35\n",
      "\u001b[m\u001b[?7h\u001b[4l\u001b>\u001b7\u001b[r\u001b[?1;3;4;6l\u001b8Preparing test data...\n",
      "Running drum using: [drum server --code-dir /Users/timothy.whittaker/Desktop/git/ford-mlops/model --address localhost:53375 --logging-level warning --show-perf --target-type binary --positive-class-label 1 --negative-class-label 0 --docker env]\n",
      "\n",
      "\n",
      "\n",
      "Running test case with timeout: 180\n",
      "Running test case: 550 bytes - 1 samples, 100 iterations\n",
      "Running test case with timeout: 180\n",
      "Running test case: 0.1MB - 190 samples, 50 iterations\n",
      "Running test case with timeout: 180\n",
      "Running test case: 10MB - 19051 samples, 5 iterations\n",
      "Running test case with timeout: 180\n",
      "Running test case: 50MB - 95256 samples, 1 iterations\n",
      "Test is done stopping drum server\n",
      "\n",
      " size    sample   iters    min     avg     max    contai   conta   conta   total\n",
      "           s                                       ner     iner    iner    physi\n",
      "                                                   used     max    limit    cal \n",
      "                                                   (MB)    used    (MB)    (MB) \n",
      "                                                           (MB)                 \n",
      "================================================================================\n",
      "550           1     100   0.065   0.070   0.096   136.79   137.3   87960   3940.\n",
      "bytes                                                  3      40   93022     852\n",
      "                                                                   207.9        \n",
      "                                                                      96        \n",
      "0.1MB       190      50   0.087   0.092   0.106   134.10   137.3   87960   3940.\n",
      "                                                       5      40   93022     852\n",
      "                                                                   207.9        \n",
      "                                                                      96        \n",
      "10MB      19051       5   0.988   1.129   1.233   174.31   205.2   87960   3940.\n",
      "                                                       2      11   93022     852\n",
      "                                                                   207.9        \n",
      "                                                                      96        \n",
      "50MB      95256       1   5.779   5.779   5.779   151.73   484.5   87960   3940.\n",
      "                                                       4      27   93022     852\n",
      "                                                                   207.9        \n",
      "                                                                      96        \n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "drum perf-test \\\n",
    "--code-dir ./model \\\n",
    "--input \"data/10K_LC_short.csv\" \\\n",
    "--target-type binary \\\n",
    "--positive-class-label 1 \\\n",
    "--negative-class-label 0 \\\n",
    "--docker env \\\n",
    "--verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: invalid mode: /opt/input.csv.\n",
      "See 'docker run --help'.\n",
      "2020-12-29 10:58:22,775 ERROR drum:  Error from docker process: 125\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/drum\", line 6, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/main.py\", line 96, in main\n",
      "    CMRunner(runtime).run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/drum.py\", line 289, in run\n",
      "    self._run_fit_and_predictions_pipelines_in_mlpiper()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/drum.py\", line 559, in _run_fit_and_predictions_pipelines_in_mlpiper\n",
      "    _pipeline_executor.run_pipeline(cleanup=False)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/pipeline/executor.py\", line 261, in run_pipeline\n",
      "    self._run_pipeline()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/pipeline/executor.py\", line 251, in _run_pipeline\n",
      "    self._dag.run_connected_pipeline(self._ml_engine)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/pipeline/dag.py\", line 138, in run_connected_pipeline\n",
      "    data_objs = dag_node.component_runner.run(parent_data_objs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/pipeline/component_runner/python_connected_component_runner.py\", line 14, in run\n",
      "    data_objs = self._dag_node.main_cls().materialize(parent_data_objs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/components/connectable_component.py\", line 11, in materialize\n",
      "    return self._materialize(parent_data_objs, self._ml_engine.user_data)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/resource/components/Python/generic_predictor/generic_predictor.py\", line 100, in _materialize\n",
      "    predictions = self._predictor.predict(input_filename)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/language_predictors/python_predictor/python_predictor.py\", line 52, in predict\n",
      "    predictions = self._model_adapter.predict(input_filename, model=self._model, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/model_adapter.py\", line 326, in predict\n",
      "    ).with_traceback(sys.exc_info()[2]) from None\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/model_adapter.py\", line 322, in predict\n",
      "    data = self._custom_hooks[CustomHooks.TRANSFORM](data, model)\n",
      "  File \"/opt/model/custom.py\", line 82, in transform\n",
      "    data = model.preprocess(data)\n",
      "  File \"/opt/model/custom_model.py\", line 37, in preprocess\n",
      "    data = process_data(self.code_dir, X)\n",
      "  File \"/opt/model/create_data.py\", line 53, in process_data\n",
      "    data = clean_up(code_dir, data)  \n",
      "  File \"/opt/model/create_data.py\", line 36, in clean_up\n",
      "    data[\"int_rate\"] = data[\"int_rate\"].apply(lambda x: x.replace(\"%\", \"\")).astype(float)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pandas/core/series.py\", line 4200, in apply\n",
      "    mapped = lib.map_infer(values, f, convert=convert_dtype)\n",
      "  File \"pandas/_libs/lib.pyx\", line 2388, in pandas._libs.lib.map_infer\n",
      "  File \"/opt/model/create_data.py\", line 36, in <lambda>\n",
      "    data[\"int_rate\"] = data[\"int_rate\"].apply(lambda x: x.replace(\"%\", \"\")).astype(float)\n",
      "AttributeError: Model transform hook failed to transform dataset: 'float' object has no attribute 'replace'\n",
      "2020-12-29 10:58:42,286 ERROR drum:  Error from docker process: 1\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/drum\", line 6, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/main.py\", line 96, in main\n",
      "    CMRunner(runtime).run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/drum.py\", line 289, in run\n",
      "    self._run_fit_and_predictions_pipelines_in_mlpiper()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/drum.py\", line 559, in _run_fit_and_predictions_pipelines_in_mlpiper\n",
      "    _pipeline_executor.run_pipeline(cleanup=False)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/pipeline/executor.py\", line 261, in run_pipeline\n",
      "    self._run_pipeline()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/pipeline/executor.py\", line 251, in _run_pipeline\n",
      "    self._dag.run_connected_pipeline(self._ml_engine)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/pipeline/dag.py\", line 138, in run_connected_pipeline\n",
      "    data_objs = dag_node.component_runner.run(parent_data_objs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/pipeline/component_runner/python_connected_component_runner.py\", line 14, in run\n",
      "    data_objs = self._dag_node.main_cls().materialize(parent_data_objs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/components/connectable_component.py\", line 11, in materialize\n",
      "    return self._materialize(parent_data_objs, self._ml_engine.user_data)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/resource/components/Python/generic_predictor/generic_predictor.py\", line 100, in _materialize\n",
      "    predictions = self._predictor.predict(input_filename)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/language_predictors/python_predictor/python_predictor.py\", line 52, in predict\n",
      "    predictions = self._model_adapter.predict(input_filename, model=self._model, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/model_adapter.py\", line 326, in predict\n",
      "    ).with_traceback(sys.exc_info()[2]) from None\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/model_adapter.py\", line 322, in predict\n",
      "    data = self._custom_hooks[CustomHooks.TRANSFORM](data, model)\n",
      "  File \"/opt/model/custom.py\", line 82, in transform\n",
      "    data = model.preprocess(data)\n",
      "  File \"/opt/model/custom_model.py\", line 37, in preprocess\n",
      "    data = process_data(self.code_dir, X)\n",
      "  File \"/opt/model/create_data.py\", line 51, in process_data\n",
      "    data = join_state_info(code_dir, data) \n",
      "  File \"/opt/model/create_data.py\", line 22, in join_state_info\n",
      "    df = data.merge(state_info, how=\"left\", left_on = [\"zip_code\", \"addr_state\"], right_on = [\"zip\", \"addr_state\"])\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\", line 7952, in merge\n",
      "    validate=validate,\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pandas/core/reshape/merge.py\", line 87, in merge\n",
      "    validate=validate,\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pandas/core/reshape/merge.py\", line 656, in __init__\n",
      "    self._maybe_coerce_merge_keys()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pandas/core/reshape/merge.py\", line 1165, in _maybe_coerce_merge_keys\n",
      "    raise ValueError(msg)\n",
      "ValueError: Model transform hook failed to transform dataset: You are trying to merge on float64 and object columns. If you wish to proceed you should use pd.concat\n",
      "2020-12-29 10:59:47,916 ERROR drum:  Error from docker process: 1\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/drum\", line 6, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/main.py\", line 96, in main\n",
      "    CMRunner(runtime).run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/drum.py\", line 289, in run\n",
      "    self._run_fit_and_predictions_pipelines_in_mlpiper()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/drum.py\", line 559, in _run_fit_and_predictions_pipelines_in_mlpiper\n",
      "    _pipeline_executor.run_pipeline(cleanup=False)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/pipeline/executor.py\", line 261, in run_pipeline\n",
      "    self._run_pipeline()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/pipeline/executor.py\", line 251, in _run_pipeline\n",
      "    self._dag.run_connected_pipeline(self._ml_engine)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/pipeline/dag.py\", line 138, in run_connected_pipeline\n",
      "    data_objs = dag_node.component_runner.run(parent_data_objs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/pipeline/component_runner/python_connected_component_runner.py\", line 14, in run\n",
      "    data_objs = self._dag_node.main_cls().materialize(parent_data_objs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mlpiper/components/connectable_component.py\", line 11, in materialize\n",
      "    return self._materialize(parent_data_objs, self._ml_engine.user_data)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/resource/components/Python/generic_predictor/generic_predictor.py\", line 100, in _materialize\n",
      "    predictions = self._predictor.predict(input_filename)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/language_predictors/python_predictor/python_predictor.py\", line 52, in predict\n",
      "    predictions = self._model_adapter.predict(input_filename, model=self._model, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/model_adapter.py\", line 326, in predict\n",
      "    ).with_traceback(sys.exc_info()[2]) from None\n",
      "  File \"/usr/local/lib/python3.7/site-packages/datarobot_drum/drum/model_adapter.py\", line 322, in predict\n",
      "    data = self._custom_hooks[CustomHooks.TRANSFORM](data, model)\n",
      "  File \"/opt/model/custom.py\", line 82, in transform\n",
      "    data = model.preprocess(data)\n",
      "  File \"/opt/model/custom_model.py\", line 37, in preprocess\n",
      "    data = process_data(self.code_dir, X)\n",
      "  File \"/opt/model/create_data.py\", line 51, in process_data\n",
      "    data = join_state_info(code_dir, data) \n",
      "  File \"/opt/model/create_data.py\", line 22, in join_state_info\n",
      "    df = data.merge(state_info, how=\"left\", left_on = [\"zip_code\", \"addr_state\"], right_on = [\"zip\", \"addr_state\"])\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\", line 7952, in merge\n",
      "    validate=validate,\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pandas/core/reshape/merge.py\", line 87, in merge\n",
      "    validate=validate,\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pandas/core/reshape/merge.py\", line 656, in __init__\n",
      "    self._maybe_coerce_merge_keys()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pandas/core/reshape/merge.py\", line 1165, in _maybe_coerce_merge_keys\n",
      "    raise ValueError(msg)\n",
      "ValueError: Model transform hook failed to transform dataset: You are trying to merge on float64 and object columns. If you wish to proceed you should use pd.concat\n",
      "2020-12-29 10:59:52,515 ERROR drum:  Error from docker process: 1\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "drum validation \\\n",
    "--code-dir ./model \\\n",
    "--input \"data/10K_LC_short.csv\" \\\n",
    "--target-type binary \\\n",
    "--positive-class-label 1 \\\n",
    "--negative-class-label 0 \\\n",
    "--docker env > validation.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the input device is not a TTY\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Validation checks results\r\n",
      "      Test case         Status                       Details                    \r\n",
      "================================================================================\r\n",
      "Null value imputation   FAILED   Null value imputation check performs check by  \r\n",
      "                                 imputing each feature with NaN value. If check \r\n",
      "                                 fails for a feature, test dataset is saved in  \r\n",
      "                                 /tmp/drum_validation_checks_. Make sure to     \r\n",
      "                                 delete those folders if it takes too much      \r\n",
      "                                 space.                                         \r\n",
      "                                                                                \r\n",
      "                                 Failed feature                          Dataset\r\n",
      "                                 filename                                       \r\n",
      "                                 ===============================================\r\n",
      "                                 =================================              \r\n",
      "                                 Unnamed: 0       /tmp/drum_validation_checks_jl\r\n",
      "                                 mb3e8g/null_value_imputation_Unna              \r\n",
      "                                                  med: 0_43oiqrw3               \r\n",
      "                                 int_rate         /tmp/drum_validation_checks_jl\r\n",
      "                                 mb3e8g/null_value_imputation_int_              \r\n",
      "                                                  rate_b4wc3i3s                 \r\n",
      "                                 zip_code         /tmp/drum_validation_checks_jl\r\n",
      "                                 mb3e8g/null_value_imputation_zip_              \r\n",
      "                                                  code_m2bnfsmd                 \r\n",
      "                                 addr_state       /tmp/drum_validation_checks_jl\r\n",
      "                                 mb3e8g/null_value_imputation_addr              \r\n",
      "                                                  _state_rno7g80i               \r\n"
     ]
    }
   ],
   "source": [
    "! tail -n 30 validation.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: looks like host DRUM version doesn't match container DRUM version. This can lead to unexpected behavior.\n",
      "Host DRUM version: drum 1.4.8\n",
      "Container DRUM version: \n",
      "the input device is not a TTY\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-04 11:03:28,052 INFO drum:  Building a docker image from directory /Users/timothy.whittaker/Desktop/git/ford-mlops/env...\n",
      "2021-01-04 11:03:28,052 INFO drum:  This may take some time\n",
      "2021-01-04 11:03:28,611 INFO drum:  Done building image!\n",
      "2021-01-04 16:03:30,708 INFO drum:  >>> Start drum in the score mode\n",
      "2021-01-04 16:03:30,709 INFO drum..Executor:  Start initializing pipeline\n",
      "2021-01-04 16:03:30,710 INFO drum..Executor:  Engine type: Generic\n",
      "2021-01-04 16:03:30,711 INFO drum..Executor:  Using python engine\n",
      "2021-01-04 16:03:30,717 INFO .ComponentsDesc:  Handling step ... engine: Generic, comp-type: generic_predictor\n",
      "2021-01-04 16:03:31,629 INFO root:  init call, code_dir -> /opt/model\n",
      "2021-01-04 16:03:31,629 INFO root:  init call, kwargs -> {}\n",
      "2021-01-04 16:03:31,630 INFO root:  Linux-4.19.76-linuxkit-x86_64-with-debian-10.7\n",
      "2021-01-04 16:03:31,630 INFO root:  ['/opt/model', '/usr/local/lib/python3.7/site-packages/datarobot_drum/resource/components/Python/generic_predictor', '/usr/local/bin', '/usr/local/lib/python37.zip', '/usr/local/lib/python3.7', '/usr/local/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/site-packages', '/opt/model']\n",
      "2021-01-04 16:03:31,764 INFO drum..Executor:  Finish initializing pipeline\n",
      "2021-01-04 16:03:31,764 INFO drum..Executor:  Start running pipeline\n",
      "2021-01-04 16:03:31,765 INFO .PythonConnectedComponentRunner:  running python connected component\n",
      "2021-01-04 16:03:31,860 INFO drum..Executor:  Finish running pipeline\n",
      "2021-01-04 16:03:31,860 INFO drum:  <<< Finish drum in the score mode\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "drum score \\\n",
    "--code-dir ./model \\\n",
    "--target-type binary \\\n",
    "--positive-class-label pos_class_prediction \\\n",
    "--negative-class-label neg_class_prediction \\\n",
    "--input data/10K_LC_short.csv \\\n",
    "--logging-level info \\\n",
    "--docker env \\\n",
    "--output data/predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_class_prediction,pos_class_prediction\r\n",
      "0.9869025253126411,0.013097474687358834\r\n",
      "0.9050606618481462,0.09493933815185383\r\n",
      "0.9795625695452991,0.020437430454700877\r\n",
      "0.9188525700336793,0.08114742996632078\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 data/predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Scoring Unstructured Model\n",
    "\n",
    "Returns Predictions and SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected score mode\n",
      "Detected /opt/model/custom.py .. trying to load hooks\n",
      "Finished loading model, total used 100 iterations\n",
      "\u001b[32m \u001b[0m\n",
      "\u001b[32m \u001b[0m\n",
      "\u001b[32m============================================================\u001b[0m\n",
      "\u001b[32mComponent: generic_predictor\u001b[0m\n",
      "\u001b[32mLanguage:  Python\u001b[0m\n",
      "\u001b[32mOutput:\u001b[0m\n",
      "\u001b[32m------------------------------------------------------------\u001b[0m\n",
      "Incoming content type params:  {'mimetype': 'application/text'}\n",
      "Incoming data type:  <class 'bytes'>\n",
      "Incoming query params:  {}\n",
      "\u001b[32m------------------------------------------------------------\u001b[0m\n",
      "\u001b[32mRuntime:    0.2 sec\u001b[0m\n",
      "\u001b[32mNR outputs: 0\u001b[0m\n",
      "\u001b[32m============================================================\u001b[0m\n",
      "\u001b[32m \u001b[0m\n",
      "docker command: [docker run --rm --interactive --user $(id -u):$(id -g)   -v /Users/timothy.whittaker/Desktop/git/ford-mlops/model:/opt/model -v \"/Users/timothy.whittaker/Desktop/git/ford-mlops/data/10K_LC_short.csv\":/opt/input.csv -v \"/Users/timothy.whittaker/Desktop/git/ford-mlops/data/predictions.json\":/opt/output.csv sha256:adbbe82490945f8bf4847f47751206900a8ecbdc672d653ea567023fff31d0cd drum score --code-dir /opt/model --input /opt/input.csv --target-type unstructured --content-type application/text --verbose --output /opt/output.csv]\n",
      "Checking DRUM version in container...\n",
      "WARNING: looks like host DRUM version doesn't match container DRUM version. This can lead to unexpected behavior.\n",
      "Host DRUM version: drum 1.4.8\n",
      "Container DRUM version: \n",
      "the input device is not a TTY\n",
      "\n",
      "--------------------\n",
      "---------- retcode: 0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "drum score \\\n",
    "--code-dir ./model \\\n",
    "--input \"data/10K_LC_short.csv\" \\\n",
    "--target-type unstructured \\\n",
    "--content-type application/text \\\n",
    "--docker env \\\n",
    "--verbose --output data/predictions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': {'negative_class_label_prob': 0.9869025253126411,\n",
      "                 'positive_class_label_prob': 0.013097474687358834},\n",
      " 'shap_values': {'POS_CLASS_SHAP_addr_state': 0.0036035509567888145,\n",
      "                 'POS_CLASS_SHAP_addr_state_name': 0.003491073098029413,\n",
      "                 'POS_CLASS_SHAP_annual_inc': 0.11969366031099128,\n",
      "                 'POS_CLASS_SHAP_delinq_2yrs': -0.011531092556902944,\n",
      "                 'POS_CLASS_SHAP_dti': 0.01487584708518544,\n",
      "                 'POS_CLASS_SHAP_earliest_cr_line day of week': -0.017397820412868293,\n",
      "                 'POS_CLASS_SHAP_earliest_cr_line month of year': -0.1550894863647559,\n",
      "                 'POS_CLASS_SHAP_emp_length': -0.11103693568966079,\n",
      "                 'POS_CLASS_SHAP_emp_title': -0.02682982281083786,\n",
      "                 'POS_CLASS_SHAP_funded_amnt': 0.026755169333029898,\n",
      "                 'POS_CLASS_SHAP_grade': 0.006084105646094981,\n",
      "                 'POS_CLASS_SHAP_home_ownership': -0.008111152884487917,\n",
      "                 'POS_CLASS_SHAP_initial_list_status': 0.0,\n",
      "                 'POS_CLASS_SHAP_inq_last_6mths': 0.024255609246618848,\n",
      "                 'POS_CLASS_SHAP_installment': 0.15164503614739766,\n",
      "                 'POS_CLASS_SHAP_int_rate': -1.1739788358980778,\n",
      "                 'POS_CLASS_SHAP_loan_amnt': -0.017080620811380016,\n",
      "                 'POS_CLASS_SHAP_mths_since_last_delinq': 0.03907136317149159,\n",
      "                 'POS_CLASS_SHAP_mths_since_last_major_derog': 0.0,\n",
      "                 'POS_CLASS_SHAP_mths_since_last_record': -0.00011018449607638986,\n",
      "                 'POS_CLASS_SHAP_open_acc': -0.05370191785775831,\n",
      "                 'POS_CLASS_SHAP_policy_code': 0.0,\n",
      "                 'POS_CLASS_SHAP_pub_rec': 0.0006438974332764621,\n",
      "                 'POS_CLASS_SHAP_purpose': -0.046163146028966734,\n",
      "                 'POS_CLASS_SHAP_pymnt_plan': 0.0,\n",
      "                 'POS_CLASS_SHAP_revol_bal': -0.028908940815859725,\n",
      "                 'POS_CLASS_SHAP_revol_util': -0.08213056965185545,\n",
      "                 'POS_CLASS_SHAP_state_id': -0.019143314686630088,\n",
      "                 'POS_CLASS_SHAP_sub_grade': -0.05322443730514654,\n",
      "                 'POS_CLASS_SHAP_term': 0.48581838170113206,\n",
      "                 'POS_CLASS_SHAP_title': -0.4410799218584213,\n",
      "                 'POS_CLASS_SHAP_total_acc': -0.09063030220777582,\n",
      "                 'POS_CLASS_SHAP_verification_status': -0.003765116930795634,\n",
      "                 'POS_CLASS_SHAP_zip_code': -0.21988152498573085}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "d = json.load(open(\"data/predictions.json\", \"rb\"))\n",
    "pprint.pprint(d[\"data\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference_server = [\n",
    "    \"drum\", \"server\",\n",
    "    \"--code-dir\",\"./model\", \n",
    "    \"--address\", \"0.0.0.0:6789\", \n",
    "    \"--show-perf\",\n",
    "    \"--target-type\", \"binary\",\n",
    "    \"--logging-level\", \"info\",\n",
    "    \"--show-stacktrace\",\n",
    "    \"--verbose\", \n",
    "    \"--docker\", \"env\"\n",
    "]\n",
    "inference_server = subprocess.Popen(run_inference_server, stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'{\"message\":\"OK\"}\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## snoozing before pinging the server to give it time to actually start\n",
    "time.sleep(5)\n",
    "print('check status')\n",
    "requests.request(\"GET\", \"http://0.0.0.0:6789/\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Server shutting down...'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## shutdown\n",
    "requests.request(\"POST\", \"http://0.0.0.0:6789/shutdown/\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Detected REST server mode - this is an advanced option\\n',\n",
       " b'Detected /opt/model/custom.py .. trying to load hooks\\n',\n",
       " b'Finished loading model, total used 100 iterations\\n',\n",
       " b'\\x1b[32m \\x1b[0m\\n',\n",
       " b'\\x1b[32m \\x1b[0m\\n',\n",
       " b'\\x1b[32m============================================================\\x1b[0m\\n',\n",
       " b'\\x1b[32mComponent: prediction_server\\x1b[0m\\n',\n",
       " b'\\x1b[32mLanguage:  Python\\x1b[0m\\n',\n",
       " b'\\x1b[32mOutput:\\x1b[0m\\n',\n",
       " b'\\x1b[32m------------------------------------------------------------\\x1b[0m\\n',\n",
       " b' * Serving Flask app \"datarobot_drum.drum.server\" (lazy loading)\\n',\n",
       " b' * Environment: production\\n',\n",
       " b'   WARNING: This is a development server. Do not use it in a production deployment.\\n',\n",
       " b'   Use a production WSGI server instead.\\n',\n",
       " b' * Debug mode: off\\n',\n",
       " b'docker command: [docker run --rm --interactive --user $(id -u):$(id -g)   -v /Users/timothy.whittaker/Desktop/git/ford-mlops/model:/opt/model -p 6789:6789 sha256:e45270890949cd2071f8822281b57de0b6a1ac394ad33a2122ec115b6b916981 drum server --code-dir /opt/model --address 0.0.0.0:6789 --show-perf --target-type binary --logging-level info --show-stacktrace --verbose]\\n',\n",
       " b'Checking DRUM version in container...\\n',\n",
       " b\"WARNING: looks like host DRUM version doesn't match container DRUM version. This can lead to unexpected behavior.\\n\",\n",
       " b'Host DRUM version: drum 1.4.8\\n',\n",
       " b'Container DRUM version: \\n',\n",
       " b'the input device is not a TTY\\n',\n",
       " b'\\n',\n",
       " b'--------------------\\n',\n",
       " b'---------- retcode: 1 ----------\\n']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_server.terminate()\n",
    "inference_server.stdout.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Artifacts and Environment to DataRobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_dir='./env', logging_level='INFO', max_wait=600, model_dir='./model', update_env=True)\n",
      "__main__ - INFO - 2020-12-17 22:54:52,191 - Namespace(env_dir='./env', logging_level='INFO', max_wait=600, model_dir='./model', update_env=True)\n",
      "__main__ - INFO - 2020-12-17 22:54:52,449 - updating environment\n",
      "__main__ - INFO - 2020-12-17 22:54:52,451 - {'description': 'this is an environment to pair with lgbm model.', 'environmentVersionID': '5fda0b242d986b76e2d33bf8', 'id': '5fda0b23d33aebb30a492f74', 'name': 'lgbm execution environment', 'programmingLanguage': 'python'}\n",
      "__main__ - INFO - 2020-12-17 22:54:52,786 - creating new execution environment version\n",
      "__main__ - INFO - 2020-12-17 22:58:09,214 - create environment version complete\n",
      "__main__ - INFO - 2020-12-17 22:58:09,214 - update env info yaml\n",
      "__main__ - INFO - 2020-12-17 22:58:09,217 - load model config\n",
      "__main__ - INFO - 2020-12-17 22:58:09,221 - {'description': None, 'environmentID': '5fda0b23d33aebb30a492f74', 'environmentVersionID': '5fdc280c844d1572968cfc5d', 'id': '5fdabcafbe83ab04a68cfc1d', 'language': 'Python', 'majorVersion': True, 'modelVersionID': '5fdc27de2319ca41b0492c43', 'name': 'lgbm-classifier', 'negativeClassLabel': 0, 'positiveClassLabel': 1, 'predictionThreshold': 0.17, 'targetName': 'is_bad', 'targetType': 'binary', 'testData': '/Users/timothy.whittaker/Desktop/git/ford-mlops/data/10K_LC_short.csv', 'testDatasetID': '5fdabcf28dce7be5c7d33b0f', 'trainingData': '/Users/timothy.whittaker/Desktop/git/ford-mlops/data/10K_Lending_Club_Loans_utf-8.csv', 'type': 'inference', 'versionId': None}\n",
      "__main__ - INFO - 2020-12-17 22:58:09,221 - grab existing inference model\n",
      "__main__ - INFO - 2020-12-17 22:58:09,503 - creating a new model version\n",
      "__main__ - INFO - 2020-12-17 22:58:16,079 - update model metadata yaml\n",
      "__main__ - INFO - 2020-12-17 22:58:16,079 - {'description': None, 'environmentID': '5fda0b23d33aebb30a492f74', 'environmentVersionID': '5fdc280c844d1572968cfc5d', 'id': '5fdabcafbe83ab04a68cfc1d', 'language': 'Python', 'majorVersion': True, 'modelVersionID': '5fdc28d22319ca41b0492cb1', 'name': 'lgbm-classifier', 'negativeClassLabel': 0, 'positiveClassLabel': 1, 'predictionThreshold': 0.17, 'targetName': 'is_bad', 'targetType': 'binary', 'testData': '/Users/timothy.whittaker/Desktop/git/ford-mlops/data/10K_LC_short.csv', 'testDatasetID': '5fdabcf28dce7be5c7d33b0f', 'trainingData': '/Users/timothy.whittaker/Desktop/git/ford-mlops/data/10K_Lending_Club_Loans_utf-8.csv', 'type': 'inference', 'versionId': None}\n"
     ]
    }
   ],
   "source": [
    "!python push.py --env-dir ./env --model-dir ./model --logging-level INFO --max-wait 600 --update-env True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model in DataRobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(logging_level='INFO', max_wait=600, model_dir='./model')\n",
      "__main__ - INFO - 2020-12-17 22:59:31,044 - Namespace(logging_level='INFO', max_wait=600, model_dir='./model')\n",
      "__main__ - INFO - 2020-12-17 22:59:31,044 - loading model config\n",
      "__main__ - INFO - 2020-12-17 22:59:31,052 - starting custom model test\n",
      "__main__ - INFO - 2020-12-17 23:07:03,930 - test complete\n",
      "__main__ - INFO - 2020-12-17 23:07:03,931 - failed ðŸ˜±\n",
      "__main__ - ERROR - 2020-12-17 23:07:03,932 - Test: error_check\n",
      "__main__ - ERROR - 2020-12-17 23:07:03,933 - Status: failed\n",
      "__main__ - ERROR - 2020-12-17 23:07:03,933 - Message: ERROR: Model transform hook failed to transform dataset: Execution failed on sql: SELECT * FROM cfds_athena_demo.us_zip_codes\n",
      "Could not connect to the endpoint URL: \"https://athena.ap-south-1.amazonaws.com/\"\n",
      "unable to rollback\n",
      "__main__ - ERROR - 2020-12-17 23:07:03,933 - Test: null_value_imputation\n",
      "__main__ - ERROR - 2020-12-17 23:07:03,933 - Status: aborted\n",
      "__main__ - ERROR - 2020-12-17 23:07:03,933 - Message: \n",
      "__main__ - ERROR - 2020-12-17 23:07:03,933 - Test: long_running_service\n",
      "__main__ - ERROR - 2020-12-17 23:07:03,933 - Status: succeeded\n",
      "__main__ - ERROR - 2020-12-17 23:07:03,933 - Message: \n",
      "__main__ - ERROR - 2020-12-17 23:07:03,933 - Test: side_effects\n",
      "__main__ - ERROR - 2020-12-17 23:07:03,933 - Status: aborted\n",
      "__main__ - ERROR - 2020-12-17 23:07:03,933 - Message: \n"
     ]
    }
   ],
   "source": [
    "!python test.py --model-dir ./model --logging-level INFO --max-wait 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
